# Asynchronous Requests

- Processing requests individually is painfully _SLOW_
- Waiting for the server to respond to our request takes the most amount of time in our applications.
- Even though we took some workload away from the server by reusing the TCP connection
- The more APIs involved, the slower the application gets. More waiting time involved.

## What do we mean when we perform our tasks _asynchornously_?

- By asynchronous, we mean that the program will switch between tasks instead of processing them one at a time. If task B can be started and we are still waiting for task A, we can do a little bit of work on task B while we wait for task A to finish.

- This does _NOT_ mean the tasks are being processed at the same time (although it may feel like it).

- Performing requests asynchronously is a way of making our application _concurrent_.

## Threading

---

Ideally we would have the following scenario: we have $N$ number of tasks $t_1, \cdots t_N$ and we would spawn a thread for each task. This poses a problem the moment we need to share resources between our threads:

Suppose we have $2$ tasks, which independently write different values to the same shared global variable `data_race`. If each task spawns a thread, what will be the value of the variable `data_race` at the end of the program?

The answer, without knowing any details of the implementation of the language is that we can't know. It may be the value of the first thread, the second, or our program may.

### Data Racing

> The situation above is an instance of a phenomenon known as _data racing_: when concurrent routines access the same shared resource and one of them writes to that resource.

### Mutex

Some languages provide mechanisms to prevent race conditions in multithreading. A popular one is called _Mutex_ or simply _Lock_.
Locks work by blocking the access to a shared resource when it is currently being accessed by another thread.
